{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b826298",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5ec4f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd7de65",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6452ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the path of the dataset\n",
    "data = pd.read_csv('archive/wiki_movie_plots_deduped.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78b0556",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "214ce67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import spacy\n",
    "import string\n",
    "import gensim\n",
    "import operator\n",
    "import re\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca0b94fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    \n",
    "    # Remove any characters that are not uppercase letters, lowercase letters, or white space character.\n",
    "    cleaned_text = re.sub(r'[^A-Za-z\\s]', '', text) \n",
    "    \n",
    "    # Replace conecutive spaces with a single space.\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3228d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize limitizers\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "891f99cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenizer(text):\n",
    "    \n",
    "    # Remove any characters that are not uppercase letters, lowercase letters, or white space character.\n",
    "    cleaned_text = re.sub(r'[^A-Za-z\\s]', '', text) \n",
    "    \n",
    "    # Replace conecutive spaces with a single space.\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    # Creating token objects      \n",
    "    tokens = word_tokenize(cleaned_text)\n",
    "    \n",
    "    \n",
    "    lowercase_tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # remove stop words\n",
    "    filtered_tokens = [token for token in lowercase_tokens if token not in stop_words]\n",
    "    \n",
    "    # limitize the tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    \n",
    "    return lemmatized_tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "103bd0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and Tokenizing...\n",
      "CPU times: total: 41.5 s\n",
      "Wall time: 1min 17s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Release Year</th>\n",
       "      <th>Title</th>\n",
       "      <th>Origin/Ethnicity</th>\n",
       "      <th>Director</th>\n",
       "      <th>Cast</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Wiki Page</th>\n",
       "      <th>Plot</th>\n",
       "      <th>plot_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1901</td>\n",
       "      <td>Kansas Saloon Smashers</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kansas_Saloon_Sm...</td>\n",
       "      <td>A bartender is working at a saloon, serving dr...</td>\n",
       "      <td>[bartender, working, saloon, serving, drink, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1901</td>\n",
       "      <td>Love by the Light of the Moon</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Love_by_the_Ligh...</td>\n",
       "      <td>The moon, painted with a smiling face hangs ov...</td>\n",
       "      <td>[moon, painted, smiling, face, hang, park, nig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1901</td>\n",
       "      <td>The Martyred Presidents</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Martyred_Pre...</td>\n",
       "      <td>The film, just over a minute long, is composed...</td>\n",
       "      <td>[film, minute, long, composed, two, shot, firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1901</td>\n",
       "      <td>Terrible Teddy, the Grizzly King</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Terrible_Teddy,_...</td>\n",
       "      <td>Lasting just 61 seconds and consisting of two ...</td>\n",
       "      <td>[lasting, second, consisting, two, shot, first...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1902</td>\n",
       "      <td>Jack and the Beanstalk</td>\n",
       "      <td>American</td>\n",
       "      <td>George S. Fleming, Edwin S. Porter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Jack_and_the_Bea...</td>\n",
       "      <td>The earliest known adaptation of the classic f...</td>\n",
       "      <td>[earliest, known, adaptation, classic, fairyta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Release Year                             Title Origin/Ethnicity  \\\n",
       "0          1901            Kansas Saloon Smashers         American   \n",
       "1          1901     Love by the Light of the Moon         American   \n",
       "2          1901           The Martyred Presidents         American   \n",
       "3          1901  Terrible Teddy, the Grizzly King         American   \n",
       "4          1902            Jack and the Beanstalk         American   \n",
       "\n",
       "                             Director Cast    Genre  \\\n",
       "0                             Unknown  NaN  unknown   \n",
       "1                             Unknown  NaN  unknown   \n",
       "2                             Unknown  NaN  unknown   \n",
       "3                             Unknown  NaN  unknown   \n",
       "4  George S. Fleming, Edwin S. Porter  NaN  unknown   \n",
       "\n",
       "                                           Wiki Page  \\\n",
       "0  https://en.wikipedia.org/wiki/Kansas_Saloon_Sm...   \n",
       "1  https://en.wikipedia.org/wiki/Love_by_the_Ligh...   \n",
       "2  https://en.wikipedia.org/wiki/The_Martyred_Pre...   \n",
       "3  https://en.wikipedia.org/wiki/Terrible_Teddy,_...   \n",
       "4  https://en.wikipedia.org/wiki/Jack_and_the_Bea...   \n",
       "\n",
       "                                                Plot  \\\n",
       "0  A bartender is working at a saloon, serving dr...   \n",
       "1  The moon, painted with a smiling face hangs ov...   \n",
       "2  The film, just over a minute long, is composed...   \n",
       "3  Lasting just 61 seconds and consisting of two ...   \n",
       "4  The earliest known adaptation of the classic f...   \n",
       "\n",
       "                                      plot_tokenized  \n",
       "0  [bartender, working, saloon, serving, drink, c...  \n",
       "1  [moon, painted, smiling, face, hang, park, nig...  \n",
       "2  [film, minute, long, composed, two, shot, firs...  \n",
       "3  [lasting, second, consisting, two, shot, first...  \n",
       "4  [earliest, known, adaptation, classic, fairyta...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Cleaning and Tokenizing...\")\n",
    "\n",
    "%time data['plot_tokenized'] = data['Plot'].map(lambda x : text_tokenizer(x))\n",
    "\n",
    "data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "644ef8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store tokens separatly \n",
    "movie_tokenized = data['plot_tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4843c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe\n",
    "data.to_csv('processed_movie_plot_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa0080",
   "metadata": {},
   "source": [
    "# Building Word Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abe98bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.58 s\n",
      "Wall time: 9.16 s\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# Build a dictionary for the tokenizd words\n",
    "%time dictionary = corpora.Dictionary(movie_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4a82e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary\n",
    "dictionary.save('movie_dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52f3421a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['appear', 0], ['assault', 1], ['bar', 2], ['bartender', 3], ['beer', 4], ['begin', 5], ['breaking', 6], ['bucket', 7], ['burst', 8], ['carrie', 9], ['cash', 10], ['customer', 11], ['drink', 12], ['dumping', 13], ['everybody', 14], ['eye', 15], ['face', 16], ['fill', 17], ['fixture', 18], ['follower', 19], ['group', 20], ['hat', 21], ['head', 22], ['inside', 23], ['irish', 24], ['leave', 25], ['man', 26], ['mirror', 27], ['nation', 28], ['order', 29], ['policeman', 30], ['pulling', 31], ['register', 32], ['saloon', 33], ['seltzer', 34], ['serving', 35], ['smashing', 36], ['spray', 37], ['stereotypically', 38], ['water', 39], ['working', 40], ['wrecking', 41], ['bench', 42], ['better', 43], ['bigger', 44], ['blocked', 45], ['causing', 46], ['couple', 47], ['embrace', 48], ['everything', 49], ['fan', 50]]]\n"
     ]
    }
   ],
   "source": [
    "# Creating a list of lists\n",
    "dict_tokens = [\n",
    "    [\n",
    "        [dictionary[key]\n",
    "         , dictionary.token2id[dictionary[key]]]\n",
    "        for key, value in dictionary.items()\n",
    "        if key <= 50\n",
    "    ]\n",
    "]\n",
    "# Printing the resulting list\n",
    "print(dict_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012745ee",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9e4f7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.72 s\n",
      "Wall time: 7.2 s\n"
     ]
    }
   ],
   "source": [
    "%time corpus = [dictionary.doc2bow(desc) for desc in movie_tokenized] # Build bag of words for the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13168780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('appear', 1), ('assault', 1), ('bar', 1), ('bartender', 2), ('beer', 2), ('begin', 1), ('breaking', 1), ('bucket', 1), ('burst', 1), ('carrie', 1), ('cash', 1), ('customer', 1), ('drink', 1), ('dumping', 1), ('everybody', 1), ('eye', 1), ('face', 1), ('fill', 1), ('fixture', 1), ('follower', 1), ('group', 2), ('hat', 1), ('head', 1), ('inside', 1), ('irish', 2), ('leave', 1), ('man', 2), ('mirror', 1), ('nation', 2), ('order', 1), ('policeman', 1), ('pulling', 1), ('register', 1), ('saloon', 1), ('seltzer', 1), ('serving', 1), ('smashing', 1), ('spray', 1), ('stereotypically', 1), ('water', 1), ('working', 1), ('wrecking', 1)], [('face', 1), ('hat', 1), ('man', 1), ('bench', 1), ('better', 1), ('bigger', 1), ('blocked', 1), ('causing', 1), ('couple', 1), ('embrace', 1), ('everything', 1), ('fan', 1), ('fence', 1), ('frown', 1), ('get', 1), ('hang', 1), ('last', 1), ('learn', 1), ('left', 1), ('look', 1), ('moon', 5), ('night', 1), ('painted', 1), ('park', 1), ('past', 1), ('perched', 1), ('railing', 1), ('scene', 1), ('see', 1), ('shoulder', 1), ('sit', 1), ('sky', 1), ('smile', 2), ('smiling', 1), ('tree', 1), ('view', 1), ('walking', 1), ('woman', 1), ('young', 1)], [('face', 1), ('altar', 2), ('assassin', 1), ('assassination', 1), ('base', 1), ('camera', 1), ('center', 1), ('composed', 1), ('display', 1), ('eight', 1), ('film', 1), ('first', 1), ('foot', 1), ('garfield', 1), ('girl', 1), ('hidden', 1), ('james', 1), ('justice', 1), ('kneel', 1), ('lady', 1), ('lincoln', 1), ('long', 2), ('mckinleyeach', 1), ('minute', 1), ('portal', 1), ('portrait', 1), ('presidentsabraham', 1), ('run', 1), ('second', 2), ('shot', 2), ('sits', 1), ('three', 1), ('tomb', 1), ('two', 1), ('u', 1), ('victim', 1), ('viewing', 1), ('william', 1)]]\n"
     ]
    }
   ],
   "source": [
    "word_frequencies = [[(dictionary[id], frequency) for id, frequency in line] for line in corpus[0:3]]\n",
    "\n",
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3044af08",
   "metadata": {},
   "source": [
    "# Tf-Idf and LSI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0508b47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_tfidf_model = gensim.models.TfidfModel(corpus, id2word=dictionary)\n",
    "\n",
    "movie_lsi_model = gensim.models.LsiModel(movie_tfidf_model[corpus], id2word=dictionary, num_topics=400)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a44d6c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "movie_tfidf_model.save('movie_tfidf_model')\n",
    "movie_lsi_model.save('movie_lsi_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a89dbf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the output of the model\n",
    "\n",
    "gensim.corpora.MmCorpus.serialize('movie_tfidf_model_mm', movie_tfidf_model[corpus])\n",
    "\n",
    "gensim.corpora.MmCorpus.serialize('movie_lsi_model_mm',movie_lsi_model[movie_tfidf_model[corpus]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "990615c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously serialized models back to memory.\n",
    "# This allows you to use the preprocessed without having to remcompute it again.\n",
    "\n",
    "\n",
    "movie_tfidf_corpus = gensim.corpora.MmCorpus('movie_tfidf_model_mm')\n",
    "movie_lsi_corpus = gensim.corpora.MmCorpus('movie_lsi_model_mm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "418315ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.similarities import MatrixSimilarity\n",
    "\n",
    "\"\"\"\n",
    "MatrixSimilarity: creates similarity index. This index allows efficient computation \n",
    "of cosine similarity between vectors in the LSI space.\n",
    "Each row in the index matrix corresponds to a document (vector in the LSI space).\n",
    "The entries of this matix are the norms of the documents.\n",
    "\"\"\"\n",
    "\n",
    "movie_index = MatrixSimilarity(movie_lsi_corpus, num_features=movie_lsi_corpus.num_terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ce3405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save similarity index\n",
    "movie_index.save('movie_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605c8d20",
   "metadata": {},
   "source": [
    "# Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6320e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def search(input_query):\n",
    "    \n",
    "    tokenized_input = text_tokenizer(input_query)\n",
    "    bow_input = dictionary.doc2bow(tokenized_input)\n",
    "    \n",
    "    query_tfidf = movie_tfidf_model[bow_input]\n",
    "    query_lsi = movie_lsi_model[query_tfidf]\n",
    "    \n",
    "    movie_index.num_best = 10\n",
    "    \n",
    "    movies_list = movie_index[query_lsi]\n",
    "    \n",
    "    \n",
    "    movies_list.sort(key=itemgetter(1), reverse=True)\n",
    "    movie_names = []\n",
    "    \n",
    "    for j, movie in enumerate(movies_list):\n",
    "\n",
    "        movie_names.append (\n",
    "            {\n",
    "                'Relevance': round((movie[1] * 100),2),\n",
    "                'Movie Title': data['Title'][movie[0]],\n",
    "                'Movie Plot': data['Plot'][movie[0]],\n",
    "                'Wikipedia Link' : data['Wiki Page'][movie[0]]\n",
    "            }\n",
    "\n",
    "        )\n",
    "        if j == (movie_index.num_best-1):\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(movie_names, columns=['Relevance','Movie Title','Movie Plot', 'Wikipedia Link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d8b9666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 125 ms\n",
      "Wall time: 194 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Relevance</th>\n",
       "      <th>Movie Title</th>\n",
       "      <th>Movie Plot</th>\n",
       "      <th>Wikipedia Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74.40</td>\n",
       "      <td>Sunset Park</td>\n",
       "      <td>Phyllis Saroka (Perlman) is a P.E. teacher at ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Sunset_Park_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73.32</td>\n",
       "      <td>Glory Road</td>\n",
       "      <td>Newly appointed men's basketball coach Don Has...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Glory_Road_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71.81</td>\n",
       "      <td>The Comebacks</td>\n",
       "      <td>Coach Lambeau Fields (David Koechner) is pathe...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Comebacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71.54</td>\n",
       "      <td>Slam Dunk: Shohoku's Greatest Challenge!</td>\n",
       "      <td>Hanamichi Sakuragi is a delinquent and the lea...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Slam_Dunk_(manga)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71.54</td>\n",
       "      <td>Slam Dunk: Howling Basketman Spirit!!</td>\n",
       "      <td>Hanamichi Sakuragi is a delinquent and the lea...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Slam_Dunk_(manga)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>70.22</td>\n",
       "      <td>Rackety Rax</td>\n",
       "      <td>Always looking for an angle, \"Knucks\" McGloin ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Rackety_Rax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>70.10</td>\n",
       "      <td>Inazuma Eleven: Saikyō Gundan Ōgre Shūrai</td>\n",
       "      <td>Endou Mamoru is a cheerful goalkeeper in Raimo...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Inazuma_Eleven_(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>69.88</td>\n",
       "      <td>The Smart Set</td>\n",
       "      <td>A self-centered polo player (Haines) has to re...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Smart_Set_(1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>69.58</td>\n",
       "      <td>Going Vertical</td>\n",
       "      <td>1970 year. The Soviet national basketball team...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Going_Vertical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>69.19</td>\n",
       "      <td>Split</td>\n",
       "      <td>A man once considered a bowling legend teams u...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Split_(2016_Sout...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Relevance                                Movie Title  \\\n",
       "0      74.40                                Sunset Park   \n",
       "1      73.32                                 Glory Road   \n",
       "2      71.81                              The Comebacks   \n",
       "3      71.54   Slam Dunk: Shohoku's Greatest Challenge!   \n",
       "4      71.54      Slam Dunk: Howling Basketman Spirit!!   \n",
       "5      70.22                                Rackety Rax   \n",
       "6      70.10  Inazuma Eleven: Saikyō Gundan Ōgre Shūrai   \n",
       "7      69.88                              The Smart Set   \n",
       "8      69.58                             Going Vertical   \n",
       "9      69.19                                      Split   \n",
       "\n",
       "                                          Movie Plot  \\\n",
       "0  Phyllis Saroka (Perlman) is a P.E. teacher at ...   \n",
       "1  Newly appointed men's basketball coach Don Has...   \n",
       "2  Coach Lambeau Fields (David Koechner) is pathe...   \n",
       "3  Hanamichi Sakuragi is a delinquent and the lea...   \n",
       "4  Hanamichi Sakuragi is a delinquent and the lea...   \n",
       "5  Always looking for an angle, \"Knucks\" McGloin ...   \n",
       "6  Endou Mamoru is a cheerful goalkeeper in Raimo...   \n",
       "7  A self-centered polo player (Haines) has to re...   \n",
       "8  1970 year. The Soviet national basketball team...   \n",
       "9  A man once considered a bowling legend teams u...   \n",
       "\n",
       "                                      Wikipedia Link  \n",
       "0   https://en.wikipedia.org/wiki/Sunset_Park_(film)  \n",
       "1    https://en.wikipedia.org/wiki/Glory_Road_(film)  \n",
       "2        https://en.wikipedia.org/wiki/The_Comebacks  \n",
       "3    https://en.wikipedia.org/wiki/Slam_Dunk_(manga)  \n",
       "4    https://en.wikipedia.org/wiki/Slam_Dunk_(manga)  \n",
       "5          https://en.wikipedia.org/wiki/Rackety_Rax  \n",
       "6  https://en.wikipedia.org/wiki/Inazuma_Eleven_(...  \n",
       "7  https://en.wikipedia.org/wiki/The_Smart_Set_(1...  \n",
       "8       https://en.wikipedia.org/wiki/Going_Vertical  \n",
       "9  https://en.wikipedia.org/wiki/Split_(2016_Sout...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time search('basketball')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92fc9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
