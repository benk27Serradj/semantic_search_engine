{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b826298",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec4f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd7de65",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6452ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the path of the dataset\n",
    "data = pd.read_csv('archive/wiki_movie_plots_deduped.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0fbfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e68788",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd169786",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ffd195",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Genre'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f88eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Origin/Ethnicity'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f8c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Release Year'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b0923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd70cab",
   "metadata": {},
   "source": [
    "## Distribution of movies over the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d98f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "movies_per_year = data['Release Year'].value_counts().sort_index().reset_index()\n",
    "movies_per_year.columns = ['Release Year' , 'Count']\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure(figsize=(13, 5))\n",
    "\n",
    "sns.lineplot(x='Release Year', y='Count', data=movies_per_year)\n",
    "\n",
    "plt.title('Distribution of the movies over the years', fontsize=25)\n",
    "plt.xlabel('Year', fontsize=20)\n",
    "plt.ylabel('Number of movies', fontsize=20)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce23b5c2",
   "metadata": {},
   "source": [
    "## Movies origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343c2cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_per_origin = data['Origin/Ethnicity'].value_counts().sort_index().reset_index()\n",
    "movies_per_origin.columns = ['Origin', 'Count']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "\n",
    "\n",
    "sns.barplot(x='Origin', y='Count', data=movies_per_origin)\n",
    "\n",
    "plt.title('Movies Origin', fontsize=25)\n",
    "plt.xlabel('Origin', fontsize=20)\n",
    "plt.ylabel('Movies Origin', fontsize=20)\n",
    "plt.xticks(fontsize=15, rotation=45, ha='right')\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78b0556",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214ce67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import spacy\n",
    "import string\n",
    "import gensim\n",
    "import operator\n",
    "import re\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0b94fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    \n",
    "    # Remove any characters that are not uppercase letters, lowercase letters, or white space character.\n",
    "    cleaned_text = re.sub(r'[^A-Za-z\\s]', '', text) \n",
    "    \n",
    "    # Replace conecutive spaces with a single space.\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3228d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize limitizers\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f99cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tokenizer(text):\n",
    "    \n",
    "    # Remove any characters that are not uppercase letters, lowercase letters, or white space character.\n",
    "    cleaned_text = re.sub(r'[^A-Za-z\\s]', '', text) \n",
    "    \n",
    "    # Replace conecutive spaces with a single space.\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    # Creating token objects      \n",
    "    tokens = word_tokenize(cleaned_text)\n",
    "    \n",
    "    \n",
    "    lowercase_tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # remove stop words\n",
    "    filtered_tokens = [token for token in lowercase_tokens if token not in stop_words]\n",
    "    \n",
    "    # limitize the tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    \n",
    "    return lemmatized_tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103bd0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaning and Tokenizing...\")\n",
    "\n",
    "%time data['plot_tokenized'] = data['Plot'].map(lambda x : nltk_tokenizer(x))\n",
    "\n",
    "data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644ef8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store tokens separatly \n",
    "movie_tokenized = data['plot_tokenized']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa0080",
   "metadata": {},
   "source": [
    "# Building Word Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe98bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# Build a dictionary for the tokenizd words\n",
    "%time dictionary = corpora.Dictionary(movie_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f3421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of lists\n",
    "dict_tokens = [\n",
    "    [\n",
    "        [dictionary[key]\n",
    "         , dictionary.token2id[dictionary[key]]]\n",
    "        for key, value in dictionary.items()\n",
    "        if key <= 50\n",
    "    ]\n",
    "]\n",
    "# Printing the resulting list\n",
    "print(dict_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012745ee",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e4f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time corpus = [dictionary.doc2bow(desc) for desc in movie_tokenized] # Build bag of words for the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13168780",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = [[(dictionary[id], frequency) for id, frequency in line] for line in corpus[0:3]]\n",
    "\n",
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3044af08",
   "metadata": {},
   "source": [
    "# Tf-Idf and LSI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0508b47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_tfidf_model = gensim.models.TfidfModel(corpus, id2word=dictionary)\n",
    "\n",
    "movie_lsi_model = gensim.models.LsiModel(movie_tfidf_model[corpus], id2word=dictionary, num_topics=400)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89dbf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the output of the model\n",
    "\n",
    "gensim.corpora.MmCorpus.serialize('movie_tfidf_model_mm', movie_tfidf_model[corpus])\n",
    "\n",
    "gensim.corpora.MmCorpus.serialize('movie_lsi_model_mm',movie_lsi_model[movie_tfidf_model[corpus]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990615c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously serialized models back to memory.\n",
    "# This allows you to use the preprocessed without having to remcompute it again.\n",
    "\n",
    "\n",
    "movie_tfidf_corpus = gensim.corpora.MmCorpus('movie_tfidf_model_mm')\n",
    "movie_lsi_corpus = gensim.corpora.MmCorpus('movie_lsi_model_mm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418315ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.similarities import MatrixSimilarity\n",
    "\n",
    "\n",
    "movie_index = MatrixSimilarity(movie_lsi_corpus, num_features=movie_lsi_corpus.num_terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605c8d20",
   "metadata": {},
   "source": [
    "# Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6320e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def search(input_query):\n",
    "    \n",
    "    tokenized_input = nltk_tokenizer(input_query)\n",
    "    bow_input = dictionary.doc2bow(tokenized_input)\n",
    "    \n",
    "    query_tfidf = movie_tfidf_model[bow_input]\n",
    "    query_lsi = movie_lsi_model[query_tfidf]\n",
    "    \n",
    "    movie_index.num_best = 10\n",
    "    \n",
    "    movies_list = movie_index[query_lsi]\n",
    "    \n",
    "    \n",
    "    movies_list.sort(key=itemgetter(1), reverse=True)\n",
    "    movie_names = []\n",
    "    \n",
    "    for j, movie in enumerate(movies_list):\n",
    "\n",
    "        movie_names.append (\n",
    "            {\n",
    "                'Relevance': round((movie[1] * 100),2),\n",
    "                'Movie Title': data['Title'][movie[0]],\n",
    "                'Movie Plot': data['Plot'][movie[0]],\n",
    "                'Wikipedia Link' : data['Wiki Page'][movie[0]]\n",
    "            }\n",
    "\n",
    "        )\n",
    "        if j == (movie_index.num_best-1):\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(movie_names, columns=['Relevance','Movie Title','Movie Plot', 'Wikipedia Link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8b9666",
   "metadata": {},
   "outputs": [],
   "source": [
    "search('basketball')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92fc9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
